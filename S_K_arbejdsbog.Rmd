---
title: "Strik & Kod"
author: "Max Odsbjerg Pedersen"
date: "1/28/2022"
output: 
    html_document:
      df_print: paged
      toc: true
      toc_depth: 2
      toc_float: true
---

```{=html}
<style>
    body { background-color: #f7a1bd; }
    p {color:white;}
    h1,h2,h3,h4,h5,h6 {color:white;}
</style>
```
Dette dokument udgør kodedelen af workshoppen Strik og Kod fra AU Library, Det Kgl. Bibliotek. Workshoppen handler om at trække paralellerne mellem strikning og kodning. "Kodning" forståes i denne sammenhæng som kodebaseret databehandling og lægger sig derfor inden for feltet data science. Som følge af workshoppens afsæt på Arts vil det nærmere blive et text mining eksempel. I text mining er man interesseret i at udtrække informationer af store mængder tekst - hvilket netop interesserer de fleste humanister.

Ingen kageopskrift uden et billede af kagen som noget af det første. Det samme gør sig gældende her. Det endelige resultat for enden af dette dokument er visualiseringen herunder, der viser hvilke ord, der bruges før forskellige ord om strikning. I grafen kan vi se at der går en pil fra "bomulds" og peger mod "strikkegarn", hvilket betyder, at det har optrådt som "bomulds garn". Jo mere markant pilen er jo flere forekomster har der været. Således kan vi altså se at et meget hyppigt forekommende ordpar er "di strikt", hvilken tydeligvis er en fejllæsning af ordet "distrikt". Dette handler om at vores data er maskingenkendt, hvilket ikke altid går lige godt. Mere om det senere.

# ![](strikke_bigrams.png) 
<br>
# Indlæsning af R-pakker

Vi arbejder i statistik-programmet R, der giver mange muligheder for statistisk arbejde og efterfølgende grafisk fremstilling af resultaterne. I R arbejder man med pakker, som tilføjer forskellige funktionaliteter til grundstammen af R-funktioner. I dette tilfælde er de relevante pakker:

```{r, message=FALSE}

```

# Data - artikler om strikning

Det første vi har brug for er noget tekstdata. Her vil vi bruge data fra den danske avissamling. Data vil blive leveret af Det Kgl. Biblioteks eksperimentielle Newspaper-API. Interaktion med API'en bygger på søgninger i Mediestream, der er Det Kgl. Biblioteks platform til søgning blandt andet i avissamlingen. Inden man kaster sig over API'en er det en kod idé at gøre sig bekendt med de udvidede søgekoder til Mediestream. Hertil kan man bruge Mediestream søgetips - <https://www2.statsbiblioteket.dk/mediestream/info/soegetips>

Se desuden konkrete søgekoder i spil her: <https://gist.github.com/maxodsbjerg/e2dd484d3c9dcaa9c422a861d6a93f6e>

Når man er tryg ved at afgrænse sine søgninger med søgekoder kan man bruge dette interface til at lave sine kald til Newspaper API'et: <http://labs.statsbiblioteket.dk/labsapi/api//api-docs?url=/labsapi/api/openapi.yaml> (Vælg "aviser/export/fields")

I denne workshop har vi forberedt et API-kald der laver følgende søgning og returnerer det matchede som data:

> strik\* AND py:[1840 TO 1850]

Denne søgning giver os altså artikler i avissamlingen i perioden 1840 til 1850, som indeholder ord der starter med "strik" og alle endelser. Derved får vi både "strikke", "strikning", "strikkede", "striktøj" og "strikketøj". Men vi får også ældre ord som "strikt" med.

I Mediestream ser søgningen således ud: ![](pics/mediestream_strik.png) Men når vi får API til at returnere data for os, så får vi den til at returnere det i CSV-format(Comma Separated Values). Den måde API'en returnerer data er ved at give os et link. Dette link vil indeholde filen med vores data. I nogle vil linket åbne filen i browseren og det vil se nogenlunde sådan her ud:

![](pics/api_strik.png) I andre tilfælde vil den downloade csv-filen ned på din computer. Det vigste er i midlertidig at API giver os et link med de rå data der passer til vores søgning. Ikke noget farvelade og interface vi kan pege og klikke på som i Mediestream-søgningen ovenfor. Det rå data kan puttes direkte ind i R, hvorefter man begynder at kunne lave sin databehandling. Lad os få vores artikler om strikning ind i R!

## Indlæs data

I kodeboksen herunder bruger vi functionen `read_csv` til at læse det link vi har fået fra API'en. Denne læsning gemmer vi i et element, som vi kalder "strik"(det kunne også være "strikke_artikler_1840_1850", men vi kommer til at referere til det mange gange fremefter, så det er fedt, at det er kort og præcist):

```{r}
strik <- read_csv("http://labs.statsbiblioteket.dk/labsapi/api/aviser/export/fields?query=strik%2A%20AND%20py%3A%5B1840%20TO%201850%5D&fields=link&fields=recordID&fields=timestamp&fields=pwa&fields=cer&fields=fulltext_org&fields=pageUUID&fields=editionUUID&fields=titleUUID&fields=editionId&fields=familyId&fields=newspaper_page&fields=newspaper_edition&fields=lplace&fields=location_name&fields=location_coordinates&max=-1&structure=header&structure=content&format=CSV")
```

Dette giver os en ny data frame i det panel der hedder "Environment" - Den hedder "strik" og vi kan se, at den har 7810 observation af 16 variable. Klik på "strik"-data framen og inspicer dit data!

Særlig interessant for os er kolonnen "fulltext_org" - det er her teksten fra artiklerne bor. Ved første øjekast skærer det dog i øjnene. Den er sprængfyldt med fejl og her er i stødt på en af faldgruberne ved at arbejde med gammel tekst: OCR-fejl.

For at forstå hvorfor disse fejl er opstået er det nødvendigt at vende blikket mod digitaliseringen. I denne proces affotograferer man aviserne (enten fra mikrofilm eller fra original), Herefter lader man en computeralgoritme løbe igennem avissiderne. Denne algoritme gør to ting: 1. Segmenterer artiklerne - med andre ord så gætter den hvilke rubrikker hører til hvilke overskrifter 2. Udfører tekstgenkendelse således, at teksten bliver digital og man kan søge i den. Dette kaldes også OCR (Optical Character Recognition)

Denne algoritme er udviklet til moderne aviser, og derfor er resultatet oftest ret godt, når man har med nyere aviser at gøre (1910 til nu). Går man længere tilbage i tiden, begynder kvaliteten på digitaliseringen at falde. Dette skyldes blandt andet, at opsætningen af aviser er en anelse forskellige fra moderne opsætning. Èn af de helt store problemer er, at tekstgenkendelsen er dårlig. Dette skyldes, at man i gamle aviser brugte frakturtyper til at trykke sine aviser. Denne tekst vil nogle kende som gotiske bogstaver eller krøllede bogstaver. ![](pics/fraktur.png) Håbet er dog imidlertidigt at vores data er så omfavnrigt, at vi alligevel kan få noget spændende ud af det.

# Indledende analyse - og lidt oprensning 

En anden af kolonner er "lplace" som angiver hvor avisen er udgivet henne. Ved først at angive hvilken data frame som vi arbejder med. Dernæst bruger vi den såkaldte pipe, `%>%`, til at sende data videre til en funktion. Her bruger vi `count`, der skal have vide hvilken kolonne den skal tælde sammen. Vi angiver her at vi vil tælde på "lplace", der ved kan vi se hvordan vores materiale fordeler sig geografisk:

```{r}

```

Ovenfor kan vi se optældningen og ikke overraskende er langt den største del af materialet fra København, der i perioden var klart den største by med flest aviser. Der dukker dog også andre interessante ting op. "Charlote Amalie" og "Christianssted" er byer på det daværende Dansk Vestindien hvorfor de altså er i avissamlingen. Disse er mestendels på engelsk og vil forstyrre i text miningen af de danske artikler om strikning. Derfor vil vi sortere dem fra, men inden vi kaster os over det, så ser vi også at Aarhus både benævnes som "Århus" og som "Aarhus". Disse to skal vi også have justeret således, at vi kun har en Aarhus i datasættet.

Lige som før starter vi med at nævne den data frame vi er interesseret i at arbejde med. Med pipen `%>%` fører vi det videre til `filter`, der filtrerer data på lplace kolonnen. Bid mærke i udråbstegnet(`!`) foran "lplace"!!!! Dette gør at den filtrere Christianssted og Charlotte Amalie fra. Uden udråbstegnet ville vi kun få tekst fra de to byer!

Dernæst piper(`%>%`) vi data videre (nu uden de to Danske Vestindiske byer) til en ny funktion. Det er en god idé at tænke på pipen som netop et rør, som man hælder data ned i, hvorefter det gribes af en funktion, der laver en ændring på data og resultatet af denne ændring kan så pipes videre igen. Den nye funktion er `mutate`, der laver ændringer på værdier i kolonner eller skaber nye kolonner. Her vil vi gerne ændre på kolonne "lplace" og den ændring får vi funktionen `str_replace` til at lave. Funktionen skal læses og siges som "string replace" og i den sammenhæng skal "string" forståes som en karakterstreng. Den finder altså en karakterstreng og erstatter den med en anden. I vores sammenhæng erstatter den "Århus" med "Aarhus".

Til sidst gemmer vi ændringer til vores dataframe "strik". Det sker til allersidst med `-> strik`. Hvis ikke man gør dette "falder" resultatet ud under kodeboksen, som vi så ovenfor, da vi talte "lplace".

```{r}

```

Vi kan nu igen tælde lplace og se, at ændringerne er som vi vil have dem (bid mærke i at resultatet falder ud under boksen, da vi *ikke* har angivet `->`:

```{r}

```

# Text mining opgaven

Data behandlingen vil tage udgangspunkt i [Tidy Data-princippet](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html) som den er implementeret i tidytext-pakken. Tankegangen er her at tage en tekst og splitte den op i enkelte ord. På denne måde optræder der kun ét ord per række i datasættet.

Det næste der sker er, at vi omdanner data til det førnævnte tidytext format, hvor hvert ord kommer til at stå på en række for sig selv, hvilket gøres med `unnest_tokens`. Resultatet af denne transformation gemmer vi til den efterfølgende analyse i "strik_tidy".

```{r}

```

Lad os lige udprinte denne nye dataframe for at se tidytext-formatet i "praksis". Dette gøres ved at skrive data framens navn:

```{r}

```

Ved at bladre i kolonner(den lille sorte pil i højre top) kan vi helt til sidst se kolonnen "word", der nu indeholder de enkelte ord.

# Analyse

## Wordcloud

For at danne os et overblik over vores datasæt vil vi nu lave en wordcloud, der indeholder de mest brugte ord i artiklerne om strikning i perioden 1840 til 1850:

```{r}

```

<br> Ikke overraskende er det småord der optræder hyppigst i datasættet. En måde at omgå disse er at indlæse en stopordliste, som man kan bruge til at fjerne stopordene:

```{r message=FALSE}
stopord <- read_csv("https://gist.githubusercontent.com/maxodsbjerg/4d1e3b1081ebba53a8d2c3aae2a1a070/raw/e1f63b4c81c15bb58a54a2f94673c97d75fe6a74/stopord_18.csv")
```

<br>

Vi prøver nu igen med funktionen anti_join(), der fjerne stopordene fra data:

```{r}

```

<br> Allerede her har vi en del interessante ord. Men der er noget der tyder på, at der er en del sammenhæng mellem tjenestepiger, der søger "condition", som i gammeldags sprogbrug er "tjenende stilling" eller en plads. Vi ser også OCR-fejlen "eondition" og den anden stavemåde "kondition".

Men hvis vi husker på at 5000 tusind af artiklerne er fra København, så er overstående i langt højere grad en undersøgelse af København. Hvordan ser den wordcloud ud, hvis vi filtrerer København fra?

```{r}

```

Nu bliver det lidt mere mystisk og ikke direkte til at gennemskue. Vidner brugen af "rbd" (rigsbanksdaler), "gode" og "par" om salgsannoncer? I håb om at komme svaret nærmere skifter vi nu fokus til de såkaldte n-grams

# N-grams

Lige som vi før fik R til at sætte alle ordene på en række for sig selv, så gør vi nu i princippet det samme. I stedet for at have ét ord pr. række vil vi dog nu have sekvenser af ord. Til at starke med tager vi sekvens er af to ord, bigrams. Disse n-grams er overlappende så i et scenarie med bigrams bliver teksten "den glade kat går ad tagryggen" til:

"den glade", "glade kat", "kat går","går ad","ad tagryggen", "tagryggen NA"

Bemærk at det sidste ord i det sidste bigram er værdien "NA". Der er altså ikke noget sidste ord i det bigram.

Ligesom før bruger vi `unnest_tokens`, men denne gang specificerer vi at vi vil have ordpar(bigrams):

```{r}

```

Lad os se det in action. Istedet for blot at skrive navnet på vores nye data frame og bladre til den sidste kolonne bruger vi nu pipen og funktionen `select` til kun at vælge vores nye kolonne:

```{r}

```

Lige som vi kunne med de enkelte ord, så kan vi også optælde bigrams:

```{r}

```

Disse bigrams fører os umibbelbart samme sted hen som før: det er opslag om tjenestepladser- og stillinger. Men hvad hvis vi igen filtrerer "København fra:

```{r}

```

Derved støder vi igen på stopordene der forstyrer os. Endnu engang vil vi gerne sortere dem fra. Denne gang i bigramsene. Før vi kan gøre det er vi dog nødt til at have splittet kolonnen "bigram" op i to: "word1", "word2":

```{r}

```

Derefter kan vi filtrere stopordene ud i begge kolonner, hvilket vi gemmer til en ny dataframe:

```{r}

```

Dernæst filtrerer vi endnu engang bigramsene fra København fra og optæller vores bigrams uden stopord

```{r}

```

En del af bigramsene vidner om salg - "faaes hos", "stort udvalg" og så videre. Interessant er "di strikt" - der tydeligvis er en OCR-fejllæsning af "distrikt". Derudover kunne bigrams som "gode anbefalinger" også tyde på at der i provinsen er tjeneste-relaterede artikler.

Eftersom vi har bigram i to kolonne kan vi nu også styre præcis hvilket ord vi kigger på som ord nummer 2. Lad os prøve med "strikke-ord". Tricket her er funktionen `str_detect`, som får at vide at den leder ord der starter med "strik" og kan efterfølges af 0 eller flere bogstaver mellem a til z og æ og ø. "\\b" angiver at det efterfølgende s skal være starten af ordet. Denne måde at angive tekstmønstre på kaldes regulære udtryk og er en kraftfuld og avanceret måde at søge efter mønstre i tekst.

```{r}

```

Vi ser stadig at "di strikt" spøger en smule, men der dukker pludselig en masse interessante bigrams op. En måde at visualisere det bedre på end en liste er gennem en netværks-graf. På listen oven for ses at flere af de hyppigt forekommende ordpar har "strikkegarn" som word2. I en netværksgraf vil strikkegarn altså blive et punkt, mens "uldent", "bomulds", "coul, og "couleurt" vil være punkter der peger ind mod "strikkegarn". På denne måde kan man på en ret overskuelig måde illustrere flere ords interne forhold.

Allerførst gemmer vi den ovenstående optælling til en ny data frame, så vi kan arbejde videre med den:

```{r}

```

Herefter bruger vi biblioteket "igraph" til at lave vores dataframe om til et netværksgraf-element. Inden da specificerer vi, at vi kun er interesserede i bigrams, der optræder mere en 4 gange:

```{r, message=FALSE}

```

Tilsidst bruger vi pakken "ggraph" til at visualisere netværket:

```{r}

```

Herved for vi altså på en overskuelig måde visualiseret de forskellige ords forhold. Men overstående er jo for artiklerne om strikning i provinsen. Hvordan ser det så ud for København hvis vi sætter ord nr 2 til at være noget med strikning (Læg mærke til at vi sætter lplace lig med København, så vi kun får resultaterne fra København:

```{r}

```

Vi ser også her at "di strikt" spøger en smule, men der dukker ligeledes en masse interessante bigrams op. Vi visualiserer igen som netværksgraf:

Som før gemmer vi den ovenstående optælling til en ny data frame:

```{r}

```

Herefter bruger vi biblioteket "igraph" til at vores dataframe om til et netværksgraf-element. Inden da specificerer vi at vi kun er interesserede i bigrams der optræder mere en 4 gange:

```{r}

```

Tilsidst bruger vi pakken "ggraph" til at visualisere netværket:

```{r}

```

For at gemme grafen bruger vi funktionen `ggsave`, hvor man angiver filnavn og type efterfulgt af bredde og højde og hvilken enhed, samt baggrundsfarven.

```{r}

```

Okay både i Jylland og København er der noget med en ny strikkebog. Hvordan går vi fra denne "distant reading"-indsigt frem til at, hvad der rent faktisk sker med den nye strikkebog. Altså bevægelsen fra distant reading til klassisk humanistisk nærlæsning. Gennem et par filtre kan vi faktisk ret hurtigt få links indtil Mediestream hvor man er tilbage helt tæt ved kilden:

```{r}

```

(Psst nr. 2 og dernæst nr 5 er interessante og sjove)

Følgende dokument er baseret på [Tidy Text Mining with R](https://www.tidytextmining.com)-bogen. Særligt kapitlerne 1 om tidyformatet generelt og 4 om n-grams. Bogen kan varmt anbefales til videre læsning og inspiration til andre undersøgelser som text mining kan.
